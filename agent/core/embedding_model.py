"""
Shared Embedding Model Manager

åŠŸèƒ½ï¼š
- å•ä¾‹æ¨¡å¼ç®¡ç† SentenceTransformer æ¨¡å‹
- é¿å… Memory å’Œ IntentRouter é‡å¤åŠ è½½æ¨¡å‹å¯¼è‡´å†…å­˜æµªè´¹
- çº¿ç¨‹å®‰å…¨çš„æ‡’åŠ è½½
"""

import logging
import threading
import time
import os
from typing import List, Optional, Any

logger = logging.getLogger(__name__)

# å…¨å±€å•ä¾‹å®ä¾‹
_shared_model_instance = None
_model_lock = threading.Lock()

class SharedEmbeddingModel:
    """
    å…±äº«åµŒå…¥æ¨¡å‹ç®¡ç†å™¨ (Singleton-ish)
    
    ä½¿ç”¨æ–¹å¼ï¼š
    model = SharedEmbeddingModel.get_instance()
    embedding = model.encode("text")
    """
    
    def __init__(self, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2"):
        self.model_name = model_name
        self._model = None
        self._ready_event = threading.Event()
        self._load_error: Optional[Exception] = None
        self._is_loading = False
        
        # ğŸ”´ CRITICAL: æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ï¼‰
        self._force_offline = os.environ.get("HF_HUB_OFFLINE", "").lower() in ("1", "true", "yes")
        if self._force_offline:
            logger.info("[SharedModel] æ£€æµ‹åˆ° HF_HUB_OFFLINE=1ï¼Œå°†å¼ºåˆ¶ä½¿ç”¨ç¦»çº¿æ¨¡å¼")
        
    @classmethod
    def get_instance(cls, model_name: str = "paraphrase-multilingual-MiniLM-L12-v2") -> 'SharedEmbeddingModel':
        """è·å–æˆ–åˆ›å»ºå…¨å±€å®ä¾‹"""
        global _shared_model_instance
        with _model_lock:
            if _shared_model_instance is None:
                _shared_model_instance = cls(model_name)
            return _shared_model_instance

    def start_loading(self):
        """è§¦å‘åå°åŠ è½½ï¼ˆå¦‚æœæ˜¯é¦–æ¬¡è°ƒç”¨ï¼‰"""
        with _model_lock:
            if self._model is not None or self._is_loading:
                return
            self._is_loading = True
            
        thread = threading.Thread(
            target=self._load_worker,
            name="SharedModelLoader",
            daemon=True
        )
        thread.start()
        logger.info("[SECURITY_SHIELD] åµŒå…¥æ¨¡å‹åå°é¢„çƒ­å·²å¯åŠ¨ï¼ˆéé˜»å¡ï¼‰")
    
    def _load_worker(self):
        """åå°åŠ è½½å·¥ä½œçº¿ç¨‹"""
        try:
            # è‡ªåŠ¨å®‰è£…ä¾èµ–
            self._ensure_dependencies()
            
            # ğŸ”´ CRITICAL: é…ç½® Hugging Face Hub ç¯å¢ƒå˜é‡ï¼Œå¢å¼ºç½‘ç»œç¨³å®šæ€§
            self._configure_hf_environment()
            
            logger.info(f"[SharedModel] å¼€å§‹åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
            start = time.time()
            
            # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¯åŠ¨æ—¶è€—æ—¶
            from sentence_transformers import SentenceTransformer
            
            # ğŸ”´ CRITICAL: ä½¿ç”¨é‡è¯•æœºåˆ¶åŠ è½½æ¨¡å‹ï¼Œå¤„ç†ç½‘ç»œé”™è¯¯
            self._model = self._load_model_with_retry(SentenceTransformer)
            
            elapsed = time.time() - start
            logger.info(f"[SharedModel] æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶ {elapsed:.1f}s")
        except Exception as e:
            logger.error(f"[SharedModel] æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
            self._load_error = e
            # ğŸ”´ CRITICAL: å³ä½¿åŠ è½½å¤±è´¥ï¼Œä¹Ÿæ ‡è®°ä¸ºå°±ç»ªï¼Œé¿å…é˜»å¡å…¶ä»–åŠŸèƒ½
            logger.warning("[SharedModel] æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼ˆæ„å›¾è·¯ç”±å¯èƒ½å—å½±å“ï¼‰")
        finally:
            self._ready_event.set()
            self._is_loading = False
    
    def _configure_hf_environment(self):
        """é…ç½® Hugging Face Hub ç¯å¢ƒå˜é‡ï¼Œå¢å¼ºç½‘ç»œç¨³å®šæ€§"""
        # è®¾ç½®æœ¬åœ°ç¼“å­˜ç›®å½•ï¼ˆé¿å…é‡å¤ä¸‹è½½ï¼‰
        cache_dir = os.path.expanduser("~/.cache/huggingface")
        os.makedirs(cache_dir, exist_ok=True)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ.setdefault("HF_HOME", cache_dir)
        os.environ.setdefault("TRANSFORMERS_CACHE", cache_dir)
        os.environ.setdefault("HF_HUB_CACHE", cache_dir)
        
        # ğŸ”´ CRITICAL: å¢åŠ è¶…æ—¶å’Œé‡è¯•é…ç½®
        os.environ.setdefault("HF_HUB_DOWNLOAD_TIMEOUT", "300")  # 5åˆ†é’Ÿè¶…æ—¶
        os.environ.setdefault("HF_HUB_ETAG_TIMEOUT", "10")  # ETag è¶…æ—¶
        
        # ç¦ç”¨è¿›åº¦æ¡ï¼ˆé¿å…è¾“å‡ºå¹²æ‰°ï¼‰
        os.environ.setdefault("HF_HUB_DISABLE_PROGRESS_BARS", "1")
        
        # ğŸ”´ CRITICAL: ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œå¦‚æœæœ¬åœ°æœ‰æ¨¡å‹åˆ™å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
        # æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰æ¨¡å‹ç¼“å­˜
        model_cache_path = os.path.join(cache_dir, "hub", "models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2")
        if os.path.exists(model_cache_path):
            logger.info("[SharedModel] æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ç¼“å­˜ï¼Œå°†ä¼˜å…ˆä½¿ç”¨ç¦»çº¿æ¨¡å¼")
            # ä¸è®¾ç½® HF_HUB_OFFLINE=1ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´æ¨¡å‹
            # ä½†å¦‚æœç½‘ç»œå¤±è´¥ï¼Œä¼šåœ¨é‡è¯•æ—¶å°è¯•ç¦»çº¿æ¨¡å¼
        
        logger.debug(f"[SharedModel] Hugging Face ç¼“å­˜ç›®å½•: {cache_dir}")
    
    def _load_model_with_retry(self, SentenceTransformer: Any, max_retries: int = 3) -> Any:
        """
        ä½¿ç”¨é‡è¯•æœºåˆ¶åŠ è½½æ¨¡å‹ï¼Œå¤„ç†ç½‘ç»œé”™è¯¯
        
        Args:
            SentenceTransformer: SentenceTransformer ç±»
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            
        Returns:
            SentenceTransformer å®ä¾‹
            
        Raises:
            Exception: å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        """
        last_error = None
        cache_folder = os.path.expanduser("~/.cache/huggingface")
        
        # ğŸ”´ CRITICAL: å¦‚æœå¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼Œç›´æ¥ä½¿ç”¨ç¦»çº¿æ¨¡å¼
        if self._force_offline:
            logger.info("[SharedModel] å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼Œç›´æ¥ä½¿ç”¨æœ¬åœ°ç¼“å­˜...")
            try:
                model = SentenceTransformer(
                    self.model_name,
                    cache_folder=cache_folder,
                    device="cpu"
                )
                logger.info("[SharedModel] âœ… ç¦»çº¿æ¨¡å¼åŠ è½½æˆåŠŸ")
                return model
            except Exception as offline_error:
                logger.error(f"[SharedModel] âŒ ç¦»çº¿æ¨¡å¼å¤±è´¥ï¼ˆæœ¬åœ°å¯èƒ½æ²¡æœ‰å®Œæ•´ç¼“å­˜ï¼‰: {offline_error}")
                raise RuntimeError(
                    f"ç¦»çº¿æ¨¡å¼åŠ è½½å¤±è´¥: {offline_error}\n"
                    f"ğŸ’¡ è¯·å…ˆåœ¨çº¿ä¸‹è½½æ¨¡å‹ï¼Œæˆ–æ‰‹åŠ¨ä¸‹è½½åˆ° ~/.cache/huggingface/"
                ) from offline_error
        
        # ğŸ”´ CRITICAL: é¦–å…ˆå°è¯•ç¦»çº¿æ¨¡å¼ï¼ˆå¦‚æœæœ¬åœ°æœ‰ç¼“å­˜ï¼‰
        try:
            logger.info("[SharedModel] é¦–å…ˆå°è¯•ç¦»çº¿æ¨¡å¼åŠ è½½ï¼ˆå¦‚æœæœ¬åœ°æœ‰ç¼“å­˜ï¼‰...")
            os.environ["HF_HUB_OFFLINE"] = "1"  # ä¸´æ—¶å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
            model = SentenceTransformer(
                self.model_name,
                cache_folder=cache_folder,
                device="cpu"
            )
            logger.info("[SharedModel] âœ… ç¦»çº¿æ¨¡å¼åŠ è½½æˆåŠŸï¼ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")
            os.environ.pop("HF_HUB_OFFLINE", None)  # æ¢å¤åœ¨çº¿æ¨¡å¼
            return model
        except Exception as offline_error:
            # ç¦»çº¿æ¨¡å¼å¤±è´¥ï¼Œç»§ç»­å°è¯•åœ¨çº¿æ¨¡å¼
            os.environ.pop("HF_HUB_OFFLINE", None)
            logger.debug(f"[SharedModel] ç¦»çº¿æ¨¡å¼å¤±è´¥ï¼ˆå¯èƒ½æ²¡æœ‰æœ¬åœ°ç¼“å­˜ï¼‰: {offline_error}")
        
        # åœ¨çº¿æ¨¡å¼é‡è¯•
        for attempt in range(1, max_retries + 1):
            try:
                logger.info(f"[SharedModel] å°è¯•åœ¨çº¿åŠ è½½æ¨¡å‹ (å°è¯• {attempt}/{max_retries})...")
                
                # ğŸ”´ CRITICAL: æ¯æ¬¡é‡è¯•éƒ½åˆ›å»ºæ–°çš„ HTTP å®¢æˆ·ç«¯ï¼ˆé¿å…å®¢æˆ·ç«¯å…³é—­é—®é¢˜ï¼‰
                # é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶åˆ›å»ºæ–°å®¢æˆ·ç«¯
                os.environ.pop("HF_HUB_OFFLINE", None)  # ç¡®ä¿åœ¨çº¿æ¨¡å¼
                
                model = SentenceTransformer(
                    self.model_name,
                    cache_folder=cache_folder,
                    device="cpu"  # å…ˆä½¿ç”¨ CPUï¼Œé¿å… MPS è®¾å¤‡é—®é¢˜
                )
                
                logger.info(f"[SharedModel] âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (å°è¯• {attempt})")
                return model
                
            except Exception as e:
                last_error = e
                error_msg = str(e)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œç›¸å…³é”™è¯¯
                is_network_error = any(keyword in error_msg.lower() for keyword in [
                    "ssl", "eof", "connection", "timeout", "closed", "http", "network", "client"
                ])
                
                if is_network_error:
                    logger.warning(f"[SharedModel] ç½‘ç»œé”™è¯¯ (å°è¯• {attempt}/{max_retries}): {error_msg[:100]}")
                    if attempt < max_retries:
                        # æŒ‡æ•°é€€é¿ï¼š2ç§’ã€4ç§’ã€8ç§’ï¼ˆå¢åŠ ç­‰å¾…æ—¶é—´ï¼‰
                        wait_time = 2 ** attempt
                        logger.info(f"[SharedModel] ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                else:
                    # éç½‘ç»œé”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    logger.error(f"[SharedModel] éç½‘ç»œé”™è¯¯ï¼Œåœæ­¢é‡è¯•: {error_msg}")
                    raise
        
        # ğŸ”´ CRITICAL: æ‰€æœ‰åœ¨çº¿é‡è¯•éƒ½å¤±è´¥ï¼Œæœ€åå°è¯•ä¸€æ¬¡ç¦»çº¿æ¨¡å¼
        logger.warning("[SharedModel] æ‰€æœ‰åœ¨çº¿é‡è¯•å¤±è´¥ï¼Œæœ€åå°è¯•ç¦»çº¿æ¨¡å¼...")
        try:
            os.environ["HF_HUB_OFFLINE"] = "1"
            model = SentenceTransformer(
                self.model_name,
                cache_folder=cache_folder,
                device="cpu"
            )
            logger.info("[SharedModel] âœ… æœ€åå°è¯•ç¦»çº¿æ¨¡å¼æˆåŠŸï¼ˆä½¿ç”¨ä¸å®Œæ•´çš„æœ¬åœ°ç¼“å­˜ï¼‰")
            os.environ.pop("HF_HUB_OFFLINE", None)
            return model
        except Exception as final_error:
            os.environ.pop("HF_HUB_OFFLINE", None)
            logger.error(f"[SharedModel] âŒ ç¦»çº¿æ¨¡å¼ä¹Ÿå¤±è´¥: {final_error}")
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        logger.error(f"[SharedModel] âŒ æ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡åœ¨çº¿ + 2 æ¬¡ç¦»çº¿")
        raise RuntimeError(
            f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡åœ¨çº¿ + 2 æ¬¡ç¦»çº¿ï¼‰: {last_error}\n"
            f"ğŸ’¡ å»ºè®®ï¼š\n"
            f"1. æ£€æŸ¥ç½‘ç»œè¿æ¥\n"
            f"2. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° ~/.cache/huggingface/\n"
            f"3. æˆ–ä½¿ç”¨ç¦»çº¿æ¨¡å¼ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ HF_HUB_OFFLINE=1"
        ) from last_error

    def _ensure_dependencies(self):
        """ç¡®ä¿ sentence-transformers å·²å®‰è£…"""
        import importlib
        import subprocess
        import sys
        
        try:
            importlib.import_module("sentence_transformers")
            return
        except ImportError:
            logger.info("[SharedModel] æœªæ£€æµ‹åˆ° sentence-transformersï¼Œå°è¯•è‡ªåŠ¨å®‰è£…...")
            try:
                subprocess.check_call([sys.executable, "-m", "pip", "install", "sentence-transformers"])
                logger.info("[SharedModel] sentence-transformers å®‰è£…æˆåŠŸ")
            except Exception as e:
                logger.error(f"[SharedModel] è‡ªåŠ¨å®‰è£…ä¾èµ–å¤±è´¥: {e}")
                raise

    def wait_until_ready(self, timeout: float = 60.0) -> bool:
        """
        ç­‰å¾…æ¨¡å‹å°±ç»ª
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            True å¦‚æœæ¨¡å‹å·²å°±ç»ªï¼ŒFalse å¦‚æœè¶…æ—¶æˆ–åŠ è½½å¤±è´¥
        """
        if self._model is not None:
            return True
        
        # ç­‰å¾…åŠ è½½å®Œæˆï¼ˆæˆ–è¶…æ—¶ï¼‰
        is_ready = self._ready_event.wait(timeout=timeout)
        
        # ğŸ”´ CRITICAL: å³ä½¿åŠ è½½å¤±è´¥ï¼Œä¹Ÿè¿”å› Trueï¼ˆé¿å…é˜»å¡ï¼‰ï¼Œä½†ä¼šåœ¨ encode æ—¶è¿”å›ç©ºåˆ—è¡¨
        # è¿™æ ·å¯ä»¥è®©ç³»ç»Ÿç»§ç»­è¿è¡Œï¼Œåªæ˜¯æ„å›¾è·¯ç”±åŠŸèƒ½ä¼šé™çº§
        return is_ready

    def encode(self, text: str) -> List[float]:
        """
        ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆå•ä¸ªæ–‡æœ¬ï¼‰
        
        Returns:
            List[float]: å‘é‡åˆ—è¡¨ã€‚å¦‚æœå‡ºé”™æˆ–æœªå°±ç»ªï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
        """
        if not self.wait_until_ready(timeout=5): # å¿«é€Ÿè¶…æ—¶ï¼Œé¿å…é˜»å¡å¤ªä¹…
            return []
            
        if self._load_error:
            return []
            
        try:
            # SentenceTransformer encode è¿”å› numpy array æˆ– tensor
            # è¿™é‡Œçš„ .tolist() ç¡®ä¿è¿”å›æ ‡å‡† list
            if self._model:
                # ä½¿ç”¨ convert_to_numpy=False é¿å…è§¦å‘æ‰¹é‡å¤„ç†è¿›åº¦æ¡
                return self._model.encode(text, convert_to_numpy=True, show_progress_bar=False).tolist()
        except Exception as e:
            logger.error(f"[SharedModel] æ¨ç†å¤±è´¥: {e}")
            return []
        
        return []
    
    def encode_batch(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡ï¼ˆç”¨äºæ‰¹é‡å¤„ç†ï¼Œæ›´é«˜æ•ˆï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            List[List[float]]: å‘é‡åˆ—è¡¨çš„åˆ—è¡¨ã€‚å¦‚æœå‡ºé”™æˆ–æœªå°±ç»ªï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
        """
        if not self.wait_until_ready(timeout=5):
            return []
            
        if self._load_error:
            return []
            
        try:
            if self._model:
                # æ‰¹é‡ç¼–ç ï¼Œä½†ç¦ç”¨è¿›åº¦æ¡ä»¥é¿å… "Batches: 100%" è¾“å‡º
                embeddings = self._model.encode(
                    texts, 
                    convert_to_numpy=True, 
                    show_progress_bar=False,  # å…³é”®ï¼šç¦ç”¨è¿›åº¦æ¡
                    batch_size=32  # åˆç†çš„æ‰¹æ¬¡å¤§å°
                )
                return [emb.tolist() for emb in embeddings]
        except Exception as e:
            logger.error(f"[SharedModel] æ‰¹é‡æ¨ç†å¤±è´¥: {e}")
            return []
        
        return []
